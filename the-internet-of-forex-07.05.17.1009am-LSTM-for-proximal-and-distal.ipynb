{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', 100)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, Quad\n",
    "from datetime import datetime\n",
    "from math import pi\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, precision_score\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Conv1D, MaxPooling1D, AveragePooling1D, UpSampling1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras import regularizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, Callback, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from weightnorm import data_based_init, AdamWithWeightnorm\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.constraints import maxnorm\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input, merge, add\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom scripts and functions\n",
    "%aimport model_performance_evaluation\n",
    "%aimport custom_metrics\n",
    "%aimport data_processing\n",
    "%aimport make_keras_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-05 15:27:11--  https://dl.dropboxusercontent.com/u/53977633/data/df072717611pm.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 133552878 (127M) [text/csv]\n",
      "Saving to: ‚Äòdf072717611pm.csv.2‚Äô\n",
      "\n",
      "df072717611pm.csv.2 100%[===================>] 127.37M  50.9MB/s    in 2.5s    \n",
      "\n",
      "2017-07-05 15:27:15 (50.9 MB/s) - ‚Äòdf072717611pm.csv.2‚Äô saved [133552878/133552878]\n",
      "\n",
      "--2017-07-05 15:27:16--  https://dl.dropboxusercontent.com/u/53977633/data/X072717611pm.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 99367127 (95M) [text/csv]\n",
      "Saving to: ‚ÄòX072717611pm.csv.2‚Äô\n",
      "\n",
      "X072717611pm.csv.2  100%[===================>]  94.76M  49.9MB/s    in 1.9s    \n",
      "\n",
      "2017-07-05 15:27:19 (49.9 MB/s) - ‚ÄòX072717611pm.csv.2‚Äô saved [99367127/99367127]\n",
      "\n",
      "--2017-07-05 15:27:19--  https://dl.dropboxusercontent.com/u/53977633/data/y072717611pm.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11428301 (11M) [text/csv]\n",
      "Saving to: ‚Äòy072717611pm.csv.2‚Äô\n",
      "\n",
      "y072717611pm.csv.2  100%[===================>]  10.90M  21.0MB/s    in 0.5s    \n",
      "\n",
      "2017-07-05 15:27:20 (21.0 MB/s) - ‚Äòy072717611pm.csv.2‚Äô saved [11428301/11428301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/u/53977633/data/df072717611pm.csv\n",
    "!wget https://dl.dropboxusercontent.com/u/53977633/data/X072717611pm.csv\n",
    "!wget https://dl.dropboxusercontent.com/u/53977633/data/y072717611pm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('/output/X072717611pm.csv')\n",
    "X.drop(X.columns[0], inplace=True, axis=1)\n",
    "y = pd.read_csv('/output/y072717611pm.csv')\n",
    "y.drop(y.columns[0], inplace=True, axis=1)\n",
    "df_ohlc_and_rectangles = pd.read_csv('/output/df072717611pm.csv')\n",
    "df_ohlc_and_rectangles.drop(df_ohlc_and_rectangles.columns[0], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = pd.read_csv('floyd-ignore/datasets/X-06.27.17.611pm.csv')\n",
    "# X.drop(X.columns[0], inplace=True, axis=1)\n",
    "# y = pd.read_csv('floyd-ignore/datasets/y-06.27.17.611pm.csv')\n",
    "# y.drop(y.columns[0], inplace=True, axis=1)\n",
    "# df_ohlc_and_rectangles = pd.read_csv('floyd-ignore/datasets/df-06.27.17.611pm.csv')\n",
    "# df_ohlc_and_rectangles.drop(df_ohlc_and_rectangles.columns[0], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data into backup X, y so we don't have to reload it \n",
    "\n",
    "X_backup = X\n",
    "y_backup = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D with residual layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = load_model('bc_conv_residual_w_dropout-jun27-401pm.h5', custom_objects={'AdamWithWeightnorm': AdamWithWeightnorm, 'fbeta_score': custom_metrics.fbeta_score, 'recall': custom_metrics.recall, 'precision': custom_metrics.precision})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds, y_preds_proba = model_performance_evaluation.eval(model, X_train, y_train, sensitivity=0.325, weights='bc_conv_residual_w_dropout-jun27-401pm.hdf5',  binary=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # For lining up predictions and labels\n",
    "# df_predictions = pd.DataFrame(preds.astype(int))\n",
    "# df_pred_test_compare = pd.concat([df_predictions, pd.DataFrame(y_train).reset_index()], \n",
    "#                          axis=1, \n",
    "#                          ignore_index=True)  \n",
    "# df_pred_test_compare.columns=['predicted', 'idx', 'true']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models for Proximal and Distal matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare data for all-at-once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset X and y\n",
    "\n",
    "X = X_backup\n",
    "y = y_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_match_list = ['Proximal_match_1_C', \n",
    "                      'Proximal_match_0_C', \n",
    "                      'Proximal_match_0_O', \n",
    "                      'Proximal_match_-1_O',\n",
    "                      'Distal_match_-1_H', \n",
    "                      'Distal_match_0_H', \n",
    "                      'Distal_match_1_H', \n",
    "                      'Distal_match_2_H', \n",
    "                      'Distal_match_-1_L', \n",
    "                      'Distal_match_0_L',\n",
    "                      'Distal_match_1_L',\n",
    "                      'Distal_match_2_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7431, 84)\n",
      "(7431, 38)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make list of features and label column names\n",
    "features = [col for col in list(X)]\n",
    "\n",
    "labels_distal = [col for col in list(y) if col.startswith('Distal')]\n",
    "labels_proximal = [col for col in list(y) if col.startswith('Proximal')]\n",
    "labels = labels_distal + labels_proximal\n",
    "\n",
    "# Select only rows that contain zones\n",
    "df_zones = df_ohlc_and_rectangles.loc[df_ohlc_and_rectangles['rectangle_here'] == 1]\n",
    "\n",
    "# Split out the new df into features and labels\n",
    "df_X = df_zones[features]\n",
    "df_y = df_zones[labels]\n",
    "\n",
    "%aimport data_processing\n",
    "# Drop features from X\n",
    "\n",
    "features_to_remove = ['Epoch Date', 'Supply/Demand', 'Object Name', 'Epoch Date Start', 'L_diff_0', 'O_diff_0',\n",
    "                      'Epoch Date End', 'Proximal', 'Distal', 'change', 'V_diff_0', 'C_diff_0', 'H_diff_0',\n",
    "                      #'O', 'H', 'L', 'C', \n",
    "                      'csv_origin_tag', 'range_high_close_range_0', 'change_open_close', 'range_high_close', \n",
    "                      'rectangle_here', 'supply_demand', 'group']\n",
    "\n",
    "df_X = data_processing.drop_non_features(df_X, features_to_remove, zero_columns=1)\n",
    "df_y = data_processing.drop_non_features(df_y, ['Distal', 'Proximal'], zero_columns=1)\n",
    "\n",
    "# Remove all columns that only contain 0s\n",
    "print(df_X.shape)\n",
    "\n",
    "# Clean up duplicate labels (i.e. where multiple matches were found)\n",
    "df_y = df_y.apply(lambda x:x.drop_duplicates(), axis=1).fillna(False)\n",
    "\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare data for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.075, random_state=0)\n",
    "# Create validation split from training\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.075, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create minmax scaled data for later autoencoder features\n",
    "# SCALE EACH FEATURE INTO [0, 1] RANGE\n",
    "sX_train = minmax_scale(X_train, axis = 0)\n",
    "sX_valid = minmax_scale(X_valid, axis = 0)\n",
    "sX_test = minmax_scale(X_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (6357, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n",
      "X:  (516, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n",
      "X:  (558, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n",
      "X:  (6357, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n",
      "X:  (516, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n",
      "X:  (558, 84, 1)\n",
      "X:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Stack to 3dim for conv net\n",
    "X_train = data_processing.X_to_Conv1D_arrays(X_train)\n",
    "X_valid = data_processing.X_to_Conv1D_arrays(X_valid)\n",
    "X_test = data_processing.X_to_Conv1D_arrays(X_test)\n",
    "\n",
    "# Stack minmax scaled to 3dim for autoencoder\n",
    "sX_train = data_processing.X_to_Conv1D_arrays(sX_train)\n",
    "sX_valid = data_processing.X_to_Conv1D_arrays(sX_valid)\n",
    "sX_test = data_processing.X_to_Conv1D_arrays(sX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original n of features: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/keras/models.py:248: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"in...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoded features: 40\n",
      "Autoencoded features + original features: 124\n"
     ]
    }
   ],
   "source": [
    "n_original_features = len(list(df_X))\n",
    "print( \"Original n of features:\", n_original_features)\n",
    "\n",
    "# Load the encoder model\n",
    "encoder = load_model('conv1d-encoder-jun30.h5', custom_objects={'AdamWithWeightnorm': AdamWithWeightnorm, 'fbeta_score': custom_metrics.fbeta_score, 'recall': custom_metrics.recall, 'precision': custom_metrics.precision})\n",
    "\n",
    "# Add a Conv1D layer to the end\n",
    "x = Conv1D(1, 3, name='back_to_3D')(encoder.layers[-1].output)\n",
    "encoder_model_input = encoder.input\n",
    "encoder = Model(input=encoder_model_input, output=[x])\n",
    "#encoder.summary()\n",
    "\n",
    "# Dimensionality reduction\n",
    "X_train_ae = encoder.predict(sX_train)\n",
    "X_valid_ae = encoder.predict(sX_valid)\n",
    "X_test_ae = encoder.predict(sX_test)\n",
    "\n",
    "print( \"Autoencoded features:\", X_train_ae.shape[1])\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train_ae), axis=1)\n",
    "X_valid = np.concatenate((X_valid, X_valid_ae), axis=1)\n",
    "X_test = np.concatenate((X_test, X_test_ae), axis=1)\n",
    "\n",
    "print( \"Autoencoded features + original features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6357, 124, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create generators\n",
    "batch_size = 64\n",
    "train_gen = make_keras_generators.train_generator(X_train, y_train.values, batch_size)\n",
    "valid_gen = make_keras_generators.valid_generator(X_valid, y_valid.values, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:3]\n",
    "\n",
    "output_shape = len(list(y_train))\n",
    "validation_steps = int(len(X_valid)/batch_size)+1\n",
    "steps_per_epoch = int(len(X_train)/batch_size)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    \n",
    "    if e > end:\n",
    "        return lr_end\n",
    "    \n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=epochs))\n",
    "mc = ModelCheckpoint('proximal-lstm-july5-1016am-mc.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "rlop = ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    mode='min',\n",
    "                    factor=0.90,\n",
    "                    patience=25, \n",
    "                    min_lr=0.00001,\n",
    "                    verbose=1,\n",
    "                        )\n",
    "\n",
    "es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.01,\n",
    "        mode='min',\n",
    "        patience=50,\n",
    "        verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [  es, rlop, mc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5880 samples, validate on 477 samples\n",
      "Epoch 1/300\n",
      "1920/5880 [========>.....................] - ETA: 51s - loss: 0.1516 - acc: 0.9518"
     ]
    }
   ],
   "source": [
    "proximal_model = Sequential()\n",
    "proximal_model.add(Bidirectional(LSTM(1024, return_sequences=False,\n",
    "                                     recurrent_dropout=0.35, dropout=0.35), \n",
    "                                 input_shape=input_shape, \n",
    "                                 batch_input_shape=(None, X_train.shape[1], X_train.shape[2])))\n",
    "proximal_model.add(Dense(output_shape, activation='sigmoid'))\n",
    "proximal_model.compile(loss='binary_crossentropy', optimizer=AdamWithWeightnorm(lr=0.01), metrics=['acc'])\n",
    "                   \n",
    "proximal_model.fit(np.array(X_train), np.array(y_train), \n",
    "                   epochs=250, \n",
    "                   batch_size=64, \n",
    "                   verbose=1, \n",
    "                   callbacks= [ mc, es, rlop ],\n",
    "                   validation_split=0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proximal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SHOW PREDICTIONS WITH NEW FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proximal_model.save('proximal-lstm-july5-1027am-final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=2)\n",
    "plt.figure(figsize=(12, 12))\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['fbeta_score'])\n",
    "plt.plot(history.history['val_fbeta_score'])\n",
    "plt.title('fbeta_score')\n",
    "plt.ylabel('fbeta_score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=2)\n",
    "plt.figure(figsize=(12, 12))\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = load_model('proximal-lstm-july4-10am', custom_objects={'AdamWithWeightnorm': AdamWithWeightnorm, 'fbeta_score': custom_metrics.fbeta_score, 'recall': custom_metrics.recall, 'precision': custom_metrics.precision})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds, preds_proba, pred_classes = model_performance_evaluation.eval(proximal_model, X_test, y_test, batch_size=16,\n",
    "                                                         temperature=0.8,\n",
    "                                                         #weights='bc_conv_residual_w_dropout-jun27-401pm.hdf5', \n",
    "                                                        sequential=1,\n",
    "                                                         binary=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on raw data and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['USDJPY1440.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allows us to easily modify how many days ahead and before we look:\n",
    "lookforward = 4 # how many days we look forward\n",
    "lookback = 7 # how many days we look back\n",
    "\n",
    "        \n",
    "# Choose features to be used in feature generation function\n",
    "feature_generation_channels = [\"O\", \"H\", \"L\", \"C\", \"V\"] \n",
    "\n",
    "X_, y_, df_ohlc_and_rectangles, groups,\\\n",
    "groups_count, scalers, groups_dict,\\\n",
    "scaled_columns, df_data_unscaled,\\\n",
    "y_rects, proximal_columns, distal_columns = data_processing.prepare_data(data, \n",
    "                                                                 lookforward, \n",
    "                                                                 lookback,\n",
    "                                                                 feature_generation_channels,\n",
    "                                                                 lag=0,\n",
    "                                                                 change_open_close=1, \n",
    "                                                                 range_high_low=1,\n",
    "                                                                 rolling_mean_range_HL=1,\n",
    "                                                                 range_high_close=1,\n",
    "                                                                 std_dev=1,\n",
    "                                                                 rolling_mean=1,\n",
    "                                                                 change_open_close_shift=1,\n",
    "                                                                 range_high_low_shift=0,\n",
    "                                                                 range_high_close_shift=1,\n",
    "                                                                 diff_shift=1,\n",
    "                                                                 rolling_mean_shift=1,\n",
    "                                                                 std_dev_shift=1,\n",
    "                                                                 center=True\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = data_processing.drop_non_features(X_, features_to_remove, zero_columns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = y_['rectangle_here']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_array, y_array = data_processing.to_Conv2D_arrays(X_,y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds, pred_classes, y_pred_proba = model_performance_evaluation.eval(conv, X_array, y_array, binary=1, keras=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For lining up predictions and data\n",
    "#df_predictions = pd.DataFrame(preds.astype(int))\n",
    "df_pred_OHLC = pd.concat([preds, \n",
    "                          pd.DataFrame(df_data_unscaled[['O', 'H', 'L', 'C']]).reset_index(drop=True)], \n",
    "                         axis=1, \n",
    "                         ignore_index=True)  \n",
    "df_pred_OHLC.columns=['predicted', 'O', 'H', 'L','C']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred_OHLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize preds and OHLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_data_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zigzag as zigzag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Dates to datetime\n",
    "df[\"Epoch Date\"] = pd.to_datetime(df[\"Epoch Date\"], unit='s')\n",
    "df[\"Epoch Date Start\"] = pd.to_datetime(df[\"Epoch Date Start\"], unit='s')\n",
    "df[\"Epoch Date End\"] = pd.to_datetime(df[\"Epoch Date End\"], unit='s')\n",
    "\n",
    "w = 12*60*60*1000 # half day in ms\n",
    "\n",
    "TOOLS = \"pan,xwheel_zoom,wheel_zoom,box_zoom,reset,save\"\n",
    "p = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_height=500, \n",
    "           plot_width=1200, title=\"üç©\", active_scroll='xwheel_zoom')\n",
    "p.xaxis.major_label_orientation = pi/4\n",
    "p.grid.grid_line_alpha=0.5\n",
    "\n",
    "p.segment(df['Epoch Date'], df.H, df['Epoch Date'], df.L, color=\"black\")\n",
    "p.vbar(df['Epoch Date'], w, df.O, df.C)\n",
    "\n",
    "# Add Rectangle data\n",
    "df_sp = df.dropna(subset=['Supply/Demand'])\n",
    "\n",
    "source_supply = ColumnDataSource(df_sp.loc[df_sp['Supply/Demand']=='SUPPLY', :])\n",
    "rect_supply = Quad(left=\"Epoch Date Start\", top=\"Proximal\", right=\"Epoch Date End\", bottom=\"Distal\", fill_color=\"firebrick\", line_alpha=0.2, fill_alpha=0.2) \n",
    "p.add_glyph(source_supply, rect_supply)\n",
    "\n",
    "source_demand = ColumnDataSource(df_sp.loc[df_sp['Supply/Demand']=='DEMAND', :])\n",
    "rect_demand = Quad(left=\"Epoch Date Start\", top=\"Proximal\", right=\"Epoch Date End\", bottom=\"Distal\", fill_color=\"steelblue\", line_alpha=0.2, fill_alpha=0.2)\n",
    "p.add_glyph(source_demand, rect_demand)\n",
    "\n",
    "\n",
    "output_file(\"candlestick.html\", title=\"candlestick data SPC test\")\n",
    "show(p)  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(n_timesteps):\n",
    "\t# create a sequence of random numbers in [0,1]\n",
    "\tX = array([random() for _ in range(n_timesteps)])\n",
    "\t# calculate cut-off value to change class values\n",
    "\tlimit = n_timesteps/4.0\n",
    "\t# determine the class outcome for each item in cumulative sequence\n",
    "\ty = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "\t# reshape input and output data to be suitable for LSTMs\n",
    "\tX = X.reshape(1, n_timesteps, 1)\n",
    "\ty = y.reshape(1, n_timesteps, 1)\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = get_sequence(50)\n",
    "print(X.shape)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
